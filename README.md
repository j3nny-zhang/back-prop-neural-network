# Neural Network Using Back Propagation

## Network Architecture
This implementation supports a neural network with three layers:

- Input layer
- Two hidden layers
- Output layer

## 

The activation functions used are:

- Sigmoid for the hidden layers
- Softmax for the output layer

## Optimization Algorithms

The project supports the following optimization algorithms:

- Stochastic Gradient Descent (SGD)
- SGD with Momentum
- AdaGrad
You can configure the optimizer by modifying the code accordingly.
